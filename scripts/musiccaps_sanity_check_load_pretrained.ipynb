{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "import argparse\n",
    "import yaml\n",
    "import torch\n",
    "\n",
    "from audioldm_train.utilities.data.dataset import MusicDataset\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning import seed_everything\n",
    "from audioldm_train.utilities.tools import get_restore_step\n",
    "from audioldm_train.utilities.model_util import instantiate_from_config\n",
    "from audioldm_train.utilities.tools import build_dataset_json_from_list\n",
    "\n",
    "from sklearn.metrics import top_k_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dummy(object):\n",
    "    pass\n",
    "\n",
    "\n",
    "args = Dummy()\n",
    "args.config_yaml = \"audioldm_train/config/2023_08_23_reproduce_audioldm/audioldm_original_medium_with_clip_clap_music.yaml\"\n",
    "args.reload_from_ckpt = \"data/checkpoints/audioldm-m-full_new.ckpt\"\n",
    "\n",
    "assert torch.cuda.is_available(), \"CUDA is not available\"\n",
    "\n",
    "config_yaml = args.config_yaml\n",
    "exp_name = os.path.basename(config_yaml.split(\".\")[0])\n",
    "exp_group_name = os.path.basename(os.path.dirname(config_yaml))\n",
    "\n",
    "config_yaml_path = os.path.join(config_yaml)\n",
    "config_yaml = yaml.load(open(config_yaml_path, \"r\"), Loader=yaml.FullLoader)\n",
    "\n",
    "if args.reload_from_ckpt != None:\n",
    "    config_yaml[\"reload_from_ckpt\"] = args.reload_from_ckpt\n",
    "dataset_json = None\n",
    "configs = config_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"seed\" in configs.keys():\n",
    "    seed_everything(configs[\"seed\"])\n",
    "else:\n",
    "    print(\"SEED EVERYTHING TO 0\")\n",
    "    seed_everything(0)\n",
    "\n",
    "if \"precision\" in configs.keys():\n",
    "    torch.set_float32_matmul_precision(configs[\"precision\"])\n",
    "\n",
    "log_path = configs[\"log_directory\"]\n",
    "\n",
    "if \"dataloader_add_ons\" in configs[\"data\"].keys():\n",
    "    dataloader_add_ons = configs[\"data\"][\"dataloader_add_ons\"]\n",
    "else:\n",
    "    dataloader_add_ons = []\n",
    "\n",
    "val_dataset = MusicDataset(\n",
    "    configs, split=\"train\", add_ons=dataloader_add_ons, dataset_json=dataset_json\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=100,\n",
    ")\n",
    "\n",
    "try:\n",
    "    config_reload_from_ckpt = configs[\"reload_from_ckpt\"]\n",
    "except:\n",
    "    config_reload_from_ckpt = None\n",
    "\n",
    "resume_from_checkpoint = config_reload_from_ckpt\n",
    "print(\"Reload ckpt specified in the config file %s\" % resume_from_checkpoint)\n",
    "\n",
    "latent_diffusion = instantiate_from_config(configs[\"model\"])\n",
    "latent_diffusion.set_log_dir(log_path, exp_group_name, exp_name)\n",
    "\n",
    "guidance_scale = configs[\"model\"][\"params\"][\"evaluation_params\"][\n",
    "    \"unconditional_guidance_scale\"\n",
    "]\n",
    "ddim_sampling_steps = configs[\"model\"][\"params\"][\"evaluation_params\"][\n",
    "    \"ddim_sampling_steps\"\n",
    "]\n",
    "n_candidates_per_samples = configs[\"model\"][\"params\"][\"evaluation_params\"][\n",
    "    \"n_candidates_per_samples\"\n",
    "]\n",
    "\n",
    "checkpoint = torch.load(resume_from_checkpoint)\n",
    "\n",
    "try:\n",
    "    latent_diffusion.load_state_dict(checkpoint[\"state_dict\"])\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    latent_diffusion.load_state_dict(checkpoint[\"state_dict\"], strict=False)\n",
    "\n",
    "latent_diffusion.eval()\n",
    "latent_diffusion = latent_diffusion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_clap = latent_diffusion.cond_stage_models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = clip_clap.three_modal_contrastive_loss(dict(\n",
    "    image=data[\"image\"].to(memory_format=torch.contiguous_format, device=\"cuda\"),\n",
    "    audio=data[\"waveform\"].to(memory_format=torch.contiguous_format, device=\"cuda\").float(),\n",
    "    text=list(data[\"text\"])\n",
    ")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import top_k_accuracy_score\n",
    "labels = results[\"labels\"]\n",
    "i2a_probs = results[\"i2a_probs\"]\n",
    "i2t_probs = results[\"i2t_probs\"]\n",
    "a2t_probs = results[\"a2t_probs\"]\n",
    "\n",
    "i2a_top3 = top_k_accuracy_score(labels, i2a_probs, k=1)\n",
    "i2t_top3 = top_k_accuracy_score(labels, i2t_probs, k=1)\n",
    "a2t_top3 = top_k_accuracy_score(labels, a2t_probs, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(i2a_top3, i2t_top3, a2t_top3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/16\", device=\"cpu\")\n",
    "model.eval()\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = data[\"image\"]\n",
    "text = data[\"text\"]\n",
    "text = clip.tokenize(text).to(\"cuda\")\n",
    "image = image.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model\n",
    "    image_features = model.encode_image(image.to(\"cuda\"))\n",
    "    text_features = model.encode_text(text.to(\"cuda\"))\n",
    "\n",
    "    logits_per_image, logits_per_text = model(image, text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(\"Label probs:\", probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_i2t_top3 = top_k_accuracy_score(labels, probs, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_i2t_top3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "image = torch.stack([preprocess(Image.open(val_dataset.data[i][\"images\"][0])) for i in range(100)])\n",
    "image = image.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model\n",
    "    image_features = model.encode_image(image.to(\"cuda\"))\n",
    "    text_features = model.encode_text(text.to(\"cuda\"))\n",
    "\n",
    "    logits_per_image, logits_per_text = model(image, text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(\"Label probs:\", probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_i2t_top3 = top_k_accuracy_score(labels, probs, k=1)\n",
    "c_i2t_top3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = preprocess(Image.open(\"CLIP.png\")).unsqueeze(0).to(device)\n",
    "text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "\n",
    "    logits_per_image, logits_per_text = model(image, text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "image = preprocess(Image.open(\"CLIP.png\")).unsqueeze(0).to(device)\n",
    "text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "\n",
    "    logits_per_image, logits_per_text = model(image, text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "image = torch.stack(\n",
    "    [preprocess(Image.open(name)).to(device) for name in [\"./man.png\", \"./woman.png\"]]\n",
    ")\n",
    "text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\", \"a man\", \"a women\"]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "\n",
    "    logits_per_image, logits_per_text = model(image, text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "\n",
    "    logits_per_image, logits_per_text = model(image, text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "\n",
    "    logits_per_image, logits_per_text = model(image, text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]\n",
    "c_i2t_top3 = top_k_accuracy_score(labels, probs, k=3)\n",
    "c_i2t_top3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audioldm_train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
